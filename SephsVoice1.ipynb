{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fcd9cd",
   "metadata": {},
   "source": [
    "---\n",
    "# Conversational Model Using the Brown Corpus\n",
    "\n",
    "**Objective**: This notebook is dedicated to constructing a sophisticated conversational model utilizing the Brown Corpus, a foundational text dataset in natural language processing. The Brown Corpus, known for its diverse range of text data, is an excellent resource for training a versatile and robust conversational model.\n",
    "\n",
    "## Key Steps:\n",
    "1. **Library Importation**: \n",
    "    - TensorFlow for building the neural network model.\n",
    "    - NLTK for advanced text processing.\n",
    "\n",
    "2. **Data Loading and Preprocessing**: \n",
    "    - Utilizing the Brown Corpus.\n",
    "    - Implementing NLTK's tokenization and stopwords filtering for data refinement.\n",
    "\n",
    "3. **Model Construction**: \n",
    "    - Designing a sequential neural network.\n",
    "    - Optimizing layers for natural language understanding.\n",
    "\n",
    "4. **Model Training and Evaluation**: \n",
    "    - Compiling and training the model.\n",
    "    - Assessing its conversational capabilities.\n",
    "\n",
    "**Goal**: To develop a well-trained model capable of engaging in a broad range of conversational contexts, demonstrating the Brown Corpus's versatility in NLP applications.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb89508-07d8-4fa6-af73-ed973dbfbdb8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install keras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf5e196",
   "metadata": {},
   "source": [
    "\n",
    "## Import Libraries\n",
    "Here we import necessary libraries such as TensorFlow and its submodules, as well as other essential Python libraries for data handling and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3690e2b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, LSTM, Dense, Bidirectional, Dropout\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from tensorflow.keras.layers import TransformerLayer # Removed this line\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "# from tensorflow.keras.layers import TransformerLayer # Removed this line\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam  # Replaced AdamW with Adam\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Additional imports for Brown Corpus processing\n",
    "import nltk\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d7d6f",
   "metadata": {},
   "source": [
    "## Load and Process the Brown Corpus\n",
    "In this section, we load the Brown Corpus, a comprehensive text dataset, leveraging the Natural Language Toolkit (NLTK) for its rich linguistic content. The Brown Corpus provides a diverse range of text data, making it ideal for training conversational models.\n",
    "\n",
    "Additionally, we employ NLTK's Punkt tokenizer for effective sentence tokenization. This tokenizer is adept at breaking text into constituent sentences, a crucial step in understanding and processing natural language.\n",
    "\n",
    "Furthermore, to refine our dataset, we incorporate the use of NLTK's English stopwords list. Stopwords are commonly used words (such as \"the\", \"is\", \"in\") that are often omitted in language processing tasks to reduce noise and focus on the meaningful content. By filtering out these stopwords, we enhance the quality of our input data, ensuring that our model learns from the most relevant linguistic elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c3ac0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the Brown Corpus\n",
    "data = brown.sents()\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Utility function to tokenize sentences and remove stopwords\n",
    "def process_sentences(data):\n",
    "    processed_text = []\n",
    "    for sentence in data:\n",
    "        words = word_tokenize(' '.join(sentence))\n",
    "        words_filtered = [word for word in words if word.lower() not in stop_words]\n",
    "        processed_text.append(' '.join(words_filtered))\n",
    "    return processed_text\n",
    "\n",
    "# Process the loaded data\n",
    "text_data = process_sentences(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5ccea-981f-46a2-a67c-5ee1366e0011",
   "metadata": {},
   "source": [
    "## Adding Sentiment Analysis to Processed Text\n",
    "Having preprocessed our text data by tokenizing and filtering out stopwords, the next crucial step is to incorporate sentiment analysis. This process will enrich our data with sentiment scores, providing a deeper understanding of the emotional context of each sentence.\n",
    "\n",
    "We use NLTK's SentimentIntensityAnalyzer to assign sentiment scores to each sentence in our processed text. These scores will be instrumental in our later stages of data analysis and modeling, allowing our model to recognize and generate responses that are sentiment-aware.\n",
    "\n",
    "The resulting data structure, `text_data_with_sentiment`, will be a list of tuples. Each tuple consists of a processed sentence and its corresponding sentiment score, combining linguistic and emotional insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5913e-880c-45a7-9437-e5741f1a5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to add sentiment analysis to processed text\n",
    "def add_sentiment_to_text(processed_text):\n",
    "    text_data_with_sentiment = []\n",
    "    for sentence in processed_text:\n",
    "        sentiment_score = sia.polarity_scores(sentence)\n",
    "        text_data_with_sentiment.append((sentence, sentiment_score))\n",
    "    return text_data_with_sentiment\n",
    "\n",
    "# Create the variable with both text data and sentiment scores\n",
    "text_data_with_sentiment = add_sentiment_to_text(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b7383",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preprocessing\n",
    "This part involves tokenizing the text data and converting it into sequences to be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9841b86",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming 'text_data_with_sentiment' is a list of tuples containing sentences and their sentiment scores\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n', lower=True)\n",
    "\n",
    "# Extracting only the text part and fit tokenizer on the text data\n",
    "texts = [item[0] for item in text_data_with_sentiment]\n",
    "tokenizer.fit_on_texts(texts)  # More efficient fitting\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Efficiently generating n-gram sequences using list comprehension\n",
    "input_sequences = [token_list[:i + 1] \n",
    "                   for line in texts \n",
    "                   for token_list in [tokenizer.texts_to_sequences([line])[0]] \n",
    "                   for i in range(1, len(token_list))]\n",
    "\n",
    "# Padding sequences to the same length\n",
    "max_sequence_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = np.array(tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Creating predictors and labels for model training\n",
    "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "label = to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae4c1a4",
   "metadata": {},
   "source": [
    "\n",
    "## Model Building\n",
    "Construct the sequential model with advanced layers like Bidirectional LSTM and TransformerLayer, suitable for language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94790e87",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, LeakyReLU\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words/2))\n",
    "model.add(LeakyReLU(alpha=0.01))  # Using LeakyReLU for better performance\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Print the model summary to check the architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d91e0",
   "metadata": {},
   "source": [
    "\n",
    "## Model Compilation\n",
    "Here, the model is compiled with a sophisticated optimizer (AdamW) and loss function for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6974780",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # Loss function for multi-class classification\n",
    "    optimizer=Adam(learning_rate=0.001),  # Adam optimizer with specified learning rate\n",
    "    metrics=['accuracy']  # Metric to monitor for performance\n",
    ")\n",
    "\n",
    "# Optional: Learning rate scheduler callback\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    factor=0.1,  # Factor by which the learning rate will be reduced\n",
    "    patience=5,  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=0.0001  # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "# Print the model summary to check the architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882b30e",
   "metadata": {},
   "source": [
    "\n",
    "## Training the Model\n",
    "The model is trained on the preprocessed data with callbacks like EarlyStopping and ModelCheckpoint for efficient learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a6816",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model_best.h5', monitor='val_accuracy', save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "# Model fitting\n",
    "model.fit(\n",
    "    predictors, \n",
    "    label, \n",
    "    epochs=50, \n",
    "    batch_size=64, \n",
    "    verbose=1, \n",
    "    callbacks=callbacks, \n",
    "    validation_split=0.1  # Using 10% of data for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0b18c",
   "metadata": {},
   "source": [
    "\n",
    "## Model Inference\n",
    "Finally, use the trained model to generate text based on a given seed text, demonstrating the model's conversational abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ecdd2ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m         seed_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m output_word\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m seed_text\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(generate_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistory shows that\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[43mmodel\u001b[49m, max_sequence_len))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len, temperature=1.0):\n",
    "    # Mapping from index to word\n",
    "    index_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predictions = model.predict(token_list)[0]\n",
    "\n",
    "        # Temperature-based sampling\n",
    "        predictions = np.asarray(predictions).astype('float64')\n",
    "        predictions = np.log(predictions + 1e-7) / temperature  # Adding a small number to avoid log(0)\n",
    "        exp_preds = np.exp(predictions)\n",
    "        predictions = exp_preds / np.sum(exp_preds)\n",
    "        predicted = np.random.choice(range(len(predictions)), p=predictions)\n",
    "\n",
    "        # Get the predicted word\n",
    "        output_word = index_word.get(predicted, '')  # Fallback to empty string if not found\n",
    "        seed_text += ' ' + output_word\n",
    "\n",
    "    return seed_text\n",
    "\n",
    "print(generate_text(\"History shows that\", 20, model, max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
